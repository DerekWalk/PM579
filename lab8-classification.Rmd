---
title: "lab8-classification"
author: "Ks"
date: "7/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
if(!require("ALL")){BiocManager::install("ALL")} 
library(ALL)
library(limma)
if(!require('MLInterfaces')) {BiocManager::install("MLInterfaces")}
library(MLInterfaces) 
if(!require("hgu95av2.db")){BiocManager::install("hgu95av2.db")} 
library(hgu95av2.db)
#if(!require('CMA')) {BiocManager::install("CMA")}
#library(CMA)
#if(!require('mlr')) install.packages("mlr")
#if(!require('caret')) install.packages("caret")
```


# MLInterfaces

These data are from a study of acute lymphoblastic leukemia (ALL). ALL has several molecular subtypes. 
```{r alldata}
data('ALL')
ALL
table(ALL$mol.biol,ALL$BT)
```

We'll study the 2 most common subtypes BCR/ABL and NEG, in B-CELL tumors.
```{r bcell}
bio <- which(ALL$mol.biol%in%c("BCR/ABL","NEG"))
isb <- grep("^B",as.character(ALL$BT))
kp <- intersect(bio,isb)
all2 <- ALL[,kp]
tmp <- all2$mol.biol=="BCR/ABL"
tmp <- ifelse(tmp,"BCR/ABL","NEG")
pData(all2)$bcrabl <- factor(tmp)
dim(all2)
```

## Feature selection

Feature selection is an issue with high-dimensional data. Too many noise (null) features will increase model complexity and lead to worse predictions. We can use a non-specific filter to reduce the model complexity, but we will do better by using features that are marginally (univariately) predictive. One strategy is to pre-select features using moderated t-tests. I will do this now to rank the features, but not to determine the number of features.  If we want to determine the number of features, that is model-fitting and needs to be done on the training data only.

```{r multtests}
des <- model.matrix(~all2$bcrabl)
fit <- lmFit(all2,des)
fit2 <- eBayes(fit)
Tdiff <- topTable(fit2,coef=2,number=500)
head(Tdiff)
```


```{r topgenes}
all2 <- all2[rownames(Tdiff),]
all2
```

## Classsification Algorithms

### 1. DLDA 

Now let's run diagonal linear discriminant analysis (DLDA). We will use the first 40 observations as the training data. The last 39 will be the test data.
```{r DLDA}
dlda1 = MLearn(bcrabl~., data = all2, .method = dldaI, trainInd = 1:40)
dlda1
confuMat(dlda1)
```

Test set misclassification rate:  6/39 (15.4\%).

Now we'll use balanced 5-fold CV. By balanced, we'll stratify the selection of the 5 data splits on the outcome variable (bcrabl).

```{r 5fcv}
dldacv5 = MLearn(bcrabl~.,all2, dldaI,
		xvalSpec("LOG",5,balKfold.xvspec(5)))
confuMat(dldacv5)
```



Rather than use all 500 features, we can perform feature selection using the features with top absolute T statistic at each cross-validation partition (option = fs.absT).  This doesn't use moderated t-tests, but our sample size is large enough, it probably makes little difference.  It also requires that we give the number of features to select, and doesn't use a p-value cutoff. Other packages (e.g. mlr) give more flexible methods for variable selection.

```{r featureselect}
dldafs = MLearn(bcrabl~.,all2, dldaI,
                 xvalSpec("LOG",5,balKfold.xvspec(5),
                          fs.absT(30) ))
dldafs
confuMat(dldafs)
```

The error is lower (7 vs 10), which suggests the less complex model is preferred. We might need to repeat this comparison to make sure the improvement is reproducible.

### 2. k-nearest neighbors

Now we'll replace DLDA with 1-NN (kNN, k=1). We'll start with the first 40 observations as the training dataset.

```{r knn}
k1=MLearn(bcrabl~.,all2,knnI(k=1),1:40)
k1
confuMat(k1)
names(RObject(k1))
```

Now we'll use 5-fold CV to estimate the test set error.
```{r knncv5}
knncv5 = MLearn(bcrabl~.,all2, knnI(k=1),
		xvalSpec("LOG",5,balKfold.xvspec(5)))
confuMat(knncv5)
```

With only 79 observations, we may prefer leave-one-out cross-validation so that our training data set is larger.

```{r loocv}
knnloocv = MLearn(bcrabl~.,all2, knnI(k=1),	xvalSpec("LOO"))
confuMat(knnloocv)
```

Does a lower prediction error mean a better model?  Unfortunately, no. The variance may be high, and we might not predict as well for an independent test set. Each training dataset is similar to the others, so similar models are estimated, which means they are all about the same as fitting all observations. We know that fitting all observations can result in a classifier with high variance and not necessarily predict well in future data sets. Newer model valdation approaches will use repeated CV, or resampling (e.g. boostrap).

### 3. Random Forest

```{r rf}
rfALL = MLearn( bcrabl~.,all2, randomForestI, 1:40 )
rfALL
confuMat(rfALL)
```

### 4. Regularized Discriminant Analysis (RDA)

```{r rda}
set.seed(5142)
r1 = MLearn(bcrabl~., all2, rdacvI, 1:40)
confuMat(r1)
```

```{r plotrda,echo=FALSE}
  plotXvalRDA(r1)  
```


```{r annotate}
psid=RObject(r1)$keptFeatures
length(psid)
psid[1:4]
psid=gsub("^X","",psid)
psid[1:4]
mget(psid[1:5],hgu95av2GENENAME)
```

```{r plotRObject, echo=FALSE}
RObject(r1)
```

```{r RObjstats}
dim(RObject(r1)$finalFit$centroids)
RObject(r1)$finalFit$centroids[1:4,]
```

# mlr: Machine Learning in R

For some reason the mlr tutorial doesn't appear on the Github website as advertised, but I found this one instead.

https://www.analyticsvidhya.com/blog/2016/08/practicing-machine-learning-techniques-in-r-with-mlr-package/

Authors are actively developing a new package mlr3. 

```{sessioninfo}
sessionInfo()
```


